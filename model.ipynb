{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect AI Generated Text data (https://www.kaggle.com/competitions/llm-detect-ai-generated-text/data)\n",
    "### DAIGT Proper Train Dataset (https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset/data)\n",
    "### DAIGT External Dataset (https://www.kaggle.com/datasets/alejopaullier/daigt-external-dataset)\n",
    "### ArguGPT (https://arxiv.org/abs/2304.07666)\n",
    "### The Imitation Game (https://arxiv.org/abs/2307.12166)\n",
    "### artem9k/ai-text-detection-pile Dataset (https://huggingface.co/datasets/artem9k/ai-text-detection-pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_39404\\1166473146.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ai_train['generated'] = 1\n",
      "C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_39404\\1166473146.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ai_test['generated'] = 1\n",
      "C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_39404\\1166473146.py:53: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_huggingFace_dataset['generated'] = df_huggingFace_dataset['generated'].replace(mapping)\n"
     ]
    }
   ],
   "source": [
    "# Detect AI Generated Text data \n",
    "df_train_essays = pd.read_csv(\"./data/train_essays.csv\")\n",
    "df_train_essays.drop_duplicates(inplace=True)\n",
    "df_train_essays.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# DAIGT Proper Train Dataset\n",
    "df_train_drcat_04 = pd.read_csv(\"./data_DAIGT/train_drcat_04.csv\")\n",
    "df_train_drcat_04.rename(columns = {\"label\":\"generated\"}, inplace=True)\n",
    "df_train_drcat_04.drop_duplicates(inplace=True)\n",
    "df_train_drcat_04.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# DAIGT External Dataset\n",
    "df_train_drcat_ext = pd.read_csv(\"./data_DAIGT/daigt_external_dataset.csv\")\n",
    "df_human = df_train_drcat_ext[['text']].rename(columns={'text': 'text'})\n",
    "df_human['generated'] = 0\n",
    "df_ai = df_train_drcat_ext[['source_text']].rename(columns={'source_text': 'text'})\n",
    "df_ai['generated'] = 1\n",
    "df_train_drcat_ext = pd.concat([df_human, df_ai], ignore_index=True) # concatenate\n",
    "df_train_drcat_ext.drop_duplicates(inplace=True)\n",
    "df_train_drcat_ext.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# ArguGPT\n",
    "df_train_arguGPT_train = pd.read_csv(\"./ArguGPT/machine-train.csv\")\n",
    "df_ai_train = df_train_arguGPT_train[['text']]\n",
    "df_ai_train['generated'] = 1\n",
    "df_train_arguGPT_test = pd.read_csv(\"./ArguGPT/machine-test.csv\")\n",
    "df_ai_test = df_train_arguGPT_test[['text']]\n",
    "df_ai_test['generated'] = 1\n",
    "df_train_arguGPT = pd.concat([df_ai_train, df_ai_test], ignore_index=True) # concatenate\n",
    "df_train_arguGPT.drop_duplicates(inplace=True)\n",
    "df_train_arguGPT.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# The Imitation game\n",
    "df_ChatGPT_essay = pd.read_csv(\"./ChatGPT/ChatGPT_essay.csv\")\n",
    "df_chatgpt = df_ChatGPT_essay[['responses']].rename(columns={'responses': 'text'})\n",
    "df_chatgpt['generated'] = 1\n",
    "df_Human_essay_1 = pd.read_csv(\"./Human/human_essay_1.csv\")\n",
    "df_Human_essay_2 = pd.read_csv(\"./Human/human_essay_2.csv\")\n",
    "df_human = pd.concat([df_Human_essay_1[[\"essays\"]].rename(columns={'essays': 'text'}), \n",
    "                      df_Human_essay_2[[\"text\"]]], ignore_index=True)\n",
    "df_human['generated'] = 0\n",
    "df_chatgpt_human = pd.concat([df_chatgpt, df_human], ignore_index=True) # concatenate\n",
    "df_chatgpt_human.drop_duplicates(inplace=True)\n",
    "df_chatgpt_human.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "# artem9k detection pile\n",
    "from datasets import load_dataset\n",
    "huggingFace_dataset = load_dataset(\"artem9k/ai-text-detection-pile\")\n",
    "df_huggingFace_dataset = huggingFace_dataset[\"train\"].to_pandas()\n",
    "df_huggingFace_dataset.drop(columns=['id'], inplace=True)\n",
    "df_huggingFace_dataset.rename(columns = {\"source\":\"generated\"}, inplace=True)\n",
    "mapping = {'human': 0, 'ai': 1}\n",
    "df_huggingFace_dataset['generated'] = df_huggingFace_dataset['generated'].replace(mapping)\n",
    "count_generated = df_huggingFace_dataset['generated'].value_counts()\n",
    "## Assuming count_generated[0] is the count for 'generated' == 0 and count_generated[1] is for 'generated' == 1\n",
    "df_huggingFace_human = df_huggingFace_dataset[df_huggingFace_dataset['generated'] == 0].head(min(100000, count_generated[0]))\n",
    "df_huggingFace_ai = df_huggingFace_dataset[df_huggingFace_dataset['generated'] == 1].head(min(100000, count_generated[1]))\n",
    "## Concatenate the two DataFrames\n",
    "df_huggingFace_dataset = pd.concat([df_huggingFace_human, df_huggingFace_ai]) # concatenate\n",
    "df_huggingFace_dataset.drop_duplicates(inplace=True)\n",
    "df_huggingFace_dataset.dropna(subset=['text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  12 Years a Slave: An Analysis of the Film Essa...          0\n",
       "1  20+ Social Media Post Ideas to Radically Simpl...          0\n",
       "2  2022 Russian Invasion of Ukraine in Global Med...          0\n",
       "3  533 U.S. 27 (2001) Kyllo v. United States: The...          0\n",
       "4  A Charles Schwab Corporation Case Essay\\n\\nCha...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_essays_final = pd.concat(\n",
    "    [df_huggingFace_dataset[[\"text\", \"generated\"]], \n",
    "     df_chatgpt_human[[\"text\", \"generated\"]], \n",
    "     df_train_arguGPT[[\"text\", \"generated\"]], \n",
    "     df_train_drcat_ext[[\"text\", \"generated\"]], \n",
    "     df_train_drcat_04[[\"text\", \"generated\"]], \n",
    "     df_train_essays[[\"text\", \"generated\"]]],\n",
    "     ignore_index=True)\n",
    "df_train_essays_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256882 entries, 0 to 256881\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       256882 non-null  object\n",
      " 1   generated  256882 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_essays_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    136226\n",
       "1    120656\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_essays_final['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# cap random characters\n",
    "def random_capitalization(paragraph): \n",
    "    return ''.join(c.upper() if random.random() < 0.2 else c for c in paragraph)\n",
    "\n",
    "# in a paragraph, manipulate a word 20% of the time, which one of the following operation\n",
    "# 1. randomly delete a character in a word\n",
    "# 2. randomly insert a char into a word\n",
    "# 3. randomly swap 2 chars in a word\n",
    "def manipulate_text(paragraph):\n",
    "    words = paragraph.split()\n",
    "    manipulated_words = []\n",
    "    for word in words:\n",
    "        if random.random() <= 0.2:\n",
    "            word = char_manipulation(word)\n",
    "        manipulated_words.append(word)\n",
    "    \n",
    "    return ' '.join(manipulated_words)\n",
    "\n",
    "def char_manipulation(word):\n",
    "    operations = [\n",
    "        lambda w: w[:idx] + w[idx+1:] if len(w) > 0 else '',  # Delete at a random char\n",
    "        lambda w: w[:idx] + random.choice('abcdefghijklmnopqrstuvwxyz') + w[idx:] if len(w) > 0 else random.choice('abcdefghijklmnopqrstuvwxyz'),  # Insert at a random index\n",
    "        lambda w: swap_random_chars(w) if len(w) > 1 else w  # Swapping two random characters\n",
    "    ]\n",
    "    idx = random.randint(0, len(word))\n",
    "    return random.choice(operations)(word)\n",
    "\n",
    "def swap_random_chars(word):\n",
    "    a, b = random.sample(range(len(word)), 2)\n",
    "    word_list = list(word)\n",
    "    word_list[a], word_list[b] = word_list[b], word_list[a]\n",
    "    return ''.join(word_list)\n",
    "\n",
    "# randomly shuffle the paragraph's sentences\n",
    "def shuffle_sentences(paragraph):\n",
    "    sentences = re.split(r'(?<=[.!?]) +', paragraph)\n",
    "    random.shuffle(sentences)\n",
    "    shuffled_paragraph = ' '.join(sentences).strip()\n",
    "    return shuffled_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_transformation(df, sample_rate, func, seed=1):\n",
    "    \"\"\"Applies a text transformation function to a fraction of rows in a DataFrame column 'text'.\"\"\"\n",
    "    # Sample indices to transform\n",
    "    sampled_indices = df.sample(frac=sample_rate, random_state=seed).index\n",
    "\n",
    "    # Copy the sampled data\n",
    "    df_augmented = df.loc[sampled_indices].copy()\n",
    "\n",
    "    # Apply transformation\n",
    "    df_augmented['text'] = df_augmented['text'].apply(func)\n",
    "\n",
    "    # Update the og df\n",
    "    df.update(df_augmented)\n",
    "\n",
    "sample_rate = 0.1\n",
    "\n",
    "# Shuffle sentences\n",
    "apply_text_transformation(df_train_essays_final, sample_rate, shuffle_sentences)\n",
    "\n",
    "# Character manipulation\n",
    "apply_text_transformation(df_train_essays_final, sample_rate, manipulate_text)\n",
    "\n",
    "# Random capitalization\n",
    "apply_text_transformation(df_train_essays_final, sample_rate, random_capitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OncE upon a time, in a quAiNt little Town nestLed amONg the roLlinG hiLls, tHere Lived a man NAmed William. \n",
      "He was BlesSed wItH SIX sons, EAch One bRinging immeasUrable joy to his lIfe. \n",
      "HIs HouSe was always a caCOphony of laughter, Mischief, aNd loVe. \n",
      "HOwEver, William NEvEr could hAve imagIned thaT his seventh soN would hOld such a Grim deStiNy.\n",
      "\n",
      "Once upon ak time, in a quaint little town nestdel among the rolling hills, there live a man ndmea William. He aws blessed with six sons, each one bringing immeasurable joy to his l.fei His house was always a cacophony of laughter, mischief, and love. However, William never could hkave imagirned that hsi seventh son ould hold such a grim destiny.\n",
      "\n",
      "He was blessed with six sons, each one bringing immeasurable joy to his life. \n",
      "However, William never could have imagined that his seventh son would hold such a grim destiny. Once upon a time, in a quaint little town nestled among the rolling hills, there lived a man named William. \n",
      "His house was always a cacophony of laughter, mischief, and love.\n"
     ]
    }
   ],
   "source": [
    "s = '''Once upon a time, in a quaint little town nestled among the rolling hills, there lived a man named William. \n",
    "He was blessed with six sons, each one bringing immeasurable joy to his life. \n",
    "His house was always a cacophony of laughter, mischief, and love. \n",
    "However, William never could have imagined that his seventh son would hold such a grim destiny.'''\n",
    "\n",
    "w = 'hello'\n",
    "\n",
    "capitalization = random_capitalization(s)\n",
    "char_mani = manipulate_text(s)\n",
    "shuffle_s = shuffle_sentences(s)\n",
    "print(capitalization)\n",
    "print()\n",
    "print(char_mani)\n",
    "print()\n",
    "print(shuffle_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_du1 = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                                num_labels=2,\n",
    "                                                                dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_du1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 512\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(df_train_essays_final[\"text\"],\n",
    "                                                    df_train_essays_final[\"generated\"],\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp,\n",
    "                                                y_tmp,\n",
    "                                                test_size=0.50,\n",
    "                                                random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        return text, label\n",
    "\n",
    "train_data = TextDataset(X_train, y_train)\n",
    "val_data = TextDataset(X_val, y_val)\n",
    "test_data = TextDataset(X_test, y_test)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    tokens = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels)\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cross-Cultural Promotion. Hazelton International Report\\n\\nIntroduction\\n\\nFor construction projects, careful planning and system thinking have a great role in effective project development and implementation. One successful function of system thinking is in the area of project management. In this case, a project is divided into manageable pieces to facilitate its completion and management, then the pieces are put back together using interface management, and the system should work properly. The important lesson is that it is not possible to optimize the parts of the system and expect that the whole system will function optimally. The optimization of the whole system takes precedence and requires that the subsystems (i.e., the manageable parts of the project) are therefore sub-optimized. It is applicable to a project, a set of projects that constitute a program, a set of programs that constitute an organizational plan, a set of organizational plans that follow selected strategies, a strategic plan that is designed to fulfill the objectives of the organization, to the mission of the organization (Frame, 2002). The main problems faced by Hazelton International are lack of strategic planning and strict schedules, lack of professional engineers and experts in the field of construction, and inadequate resource allocation and budgeting.\\n\\nProblem Identification\\n\\nThe case study shows that Hazelton International deals with a number of interrelated technologies. The introduction of new technology by Hazelton International has a very sharp impact on each of the current core technologies, especially on their interdependencies. The main problems identified to deal with ineffective strategy planning and ineffective project management. The main problems involve inadequate equipment delivered from Canada, slow construction processes (bridges), lack of coordination between national authorities and the construction company, high oil prices, and poor budgeting and forecasting. The overall management system should function properly if there is a connection between all levels of management (Owens and Wilson 1996). Local subsidiaries as a subsystem to the overall system need not be optimized for the overall organization to be optimized. In practice, neither the management system nor the R&D management system is linear. The system is an iterative system with feedback loops at every level (Frame, 2002).\\n\\nFor Hazelton International, the strategy of allocating resources to R&D should be based on the strategic planning of the organization designed to accomplish a three-pronged attack. The problem is that its resources were provided to long-term activities of R&D that were ongoing, provided they were still in line with the strategic plan of the organization. New long-term activities were not allocated. The budgeting function was misunderstood by managers, but the blame for the misunderstanding was partially due to the financial function of the enterprise (Owens and Wilson 1996). The financial people looked at budgets as sacred and at any deviation from the budget as the original sin. Budgeting in general, and budgeting in particular, is a process or a tool that allocates resources to activities with high uncertainty. There is no reason for the company organization to remain on a budget if the objectives of R&D are not accomplished. R&D can be very efficient but totally ineffective, yet the finance people will be happy. Overspending resources as necessary but giving the organization the opportunity to corner the market must be the hallmark of all organizations (Burkun, 2005).\\n\\nPoor cross-cultural management was another problem for the company. People in America tend to be future-oriented rather than oriented to the present or the past. It is one of their fundamental assumptions that the capacity to defer gratification (doing something that is not particularly pleasant today to further future pleasure) is a positive indication. In short, Americans assume that the environment can be subjugated to the human will, given enough time, effort and money. And it should — in the service of humankind (Owens and Wilson 1996). This exploitative attitude toward nature has led to a material richness of American society, no doubt, but such a culture also ranks preservation of the environment as only a tertiary value. The native traditions differ greatly from American values and are based on Muslim religious doctrines and principles (Frame, 2002).\\n\\nAnalysis\\n\\nMany objectives of Hazelton International are nonspecific and general. If management is satisfied that the goals are appropriate, then it is necessary to develop strategies to allow these goals to be completed successfully. At this point, there should be an analysis of the various strategic options available to the organization and a choice of the strategies most likely to be successful while giving the organization a competitive edge in the marketplace (Owens and Wilson 1996). The introduction of a monitoring system will help the company to meet plans and schedules. It is also a means to control the performance of the organization once discrepancies in performance are detected by the monitoring. Appropriate corrective actions, if warranted, can be implemented, monitored, and controlled. This means that the plans must support the strategies, the strategies must accomplish the goals, and the goals must result in objectives that support the mission. If this is not the case, then adjustments must be introduced to plans, strategies, goals, objectives, and/ or mission, in that order (Frame, 2002).\\n\\nAlternatives\\n\\nIf management is satisfied that the goals are appropriate, then it is necessary to develop strategies to allow these goals to be completed successfully. At this point, there should be an analysis of the various strategic options available to the organization and a choice of the strategies most likely to be successful while giving the organization a competitive edge in the marketplace. Using the chosen strategies, the organization develops plans of action that will fulfill the goals, objectives, and mission of the organization (Owens and Wilson 1996). The introduction of a monitoring system indicates that an organization is a dynamic entity that is affected by internal and external environments, and it, therefore, must adjust plans as new information is obtained, new regulations are enacted, and new or old competitors pervade the marketplace (Burkun, 2005). It is also a means to control the performance of the organization once discrepancies in performance are detected by the monitoring. Appropriate corrective actions, if warranted, can be implemented, monitored, and controlled. The second level of decision-making is the strategic level, where the objectives of the organizations are translated into goals with specific results to be accomplished. Input from new plans, even if only technical, is a must and constitutes a “reality check” of general management’s knowledge of technical incompatibilities and conceptual incongruities (Frame, 2002).\\n\\nSpecial attention should be given to cultural differences and variations. One aspect of controlling today for the future is that managers in the American type of culture are more involved in strategy and that they categorize strategic issues as opportunities, while some other cultures overwhelmingly view them as problems. Religion relates to all aspects of life among Muslims. It also has a major impact on thinking. To gain access to this divine reality, there are many rules to follow for a Muslim. Religion becomes part of a Muslim’s daily life, and a visitor to the world who shows respect for the Islamic religion will gain a favorable reception almost everywhere. This means, among other things, refraining from drinking alcohol at social events and not exposing any kind of images, such as religious symbols, statues, and so on. This also means that the visitor, whether a businessman or not, encounters a male-dominated society. Women are usually not part of the entertainment scene in the Muslim world (Adler, 2008).\\n\\nTraining of staff will be, in essence, the management of change and thus must include forecasts to be able to impact the future of the organization. The management, more than any other function in the organization, needs to manage its operation on a day-to-day basis. However, it must also be aware of alternative views of the future to position the organization ahead of the competition. These alternative views of the future (often called scenarios of the future) introduce a time parameter in system thinking. System thinking requires that predetermined objectives be set to determine if the work is on track and fits with the other tasks set to successfully accomplish the objectives of the organization (Adler, 2008). Managers feel constrained by predetermined objectives, especially if these have been predetermined by management, and worst of all, by general management. The “common wisdom” in R&D circles, and this appears to be repeated in the literature of management, is that attempts to guide research toward predetermined objectives are doomed to failure. The new approach does not subscribe to that common wisdom, which is still alive and doing well in academia and in some government laboratories. Result-oriented R&D requires predetermined objectives to be effective and efficient (Frame, 2002).\\n\\nBudgets are essential guidelines for the utilization of allocated resources. They are also a control mechanism that monitors the implementation and completion of the strategic plan using financial indicators. Nonfinancial indicators such as materials, labor hours, sales, and production can be translated into financial terms allowing for comparative analysis (Adler, 2008). The manager of Hazelton International must understand the objectives of budgets and must communicate these objectives to the scientists in order to obtain their commitment to the organization. Training expenditures could be included in the R&D budget but are often included under the human resources budget (Burkun, 2005). The controlling mechanism occurs by comparing the actual expenditures against the budget on a monthly basis or any other time basis that is appropriate. Construction managers receive a printout for their organizational units, and each unit reporting to the construction manager also receives its appropriate printouts. Comparisons of actual versus budgeted activities are done and expressed in terms of variances (Owens and Wilson 1996).\\n\\nConclusion\\n\\nThe case of Hazelton International shows that lack of strategic planning and thinking leads to project failure. Hazelton International should have a mission that is either a formal statement or an informal one. The people dimension in R&D looms as one of the most critical factors involved in the management of that function. This dimension is the least understood by general management and by managers of R&D. The principle of system thinking in R&D management should be given primary concern to the evaluation of teams’ performances. However, there is a general feeling in R&D management that each individual should be evaluated in isolation from other individual scientists. The emergence of multidisciplinary teams in most healthcare research organizations is in direct opposition to the principle of individual performance appraisal, which still persists in the industry. Theoretically, the performance of an individual scientist on a project team can never be optimized since the performance of the team, according to the concept of system thinking, is prime. The diversity of the workforce in organizations is an additional issue that can affect management. The current workforce is not as homogeneous as it used to be, so it requires different management approaches. Thus the manager must be sensitive to differences in behavior, attitudes, and responses to such things as motivational principles and emergencies. The top manager should take advantage of the diversity of the workforce and not consider it as a problem.\\n\\nPlan Of Action\\n\\nNew Plan Development\\n\\n 1. New budgeting b. training of employees (expertise and cross-cultural training)\\n 2. Agreement with local governments (concerning compensation for crops)\\n 3. Plan Implementation\\n 4. Strict Scheduling b. Control Group\\n 5. Support and Coordination from the Main office\\n\\nThe magnitude of the variance from the perspective of the manager is important, especially in activities with a high level of uncertainty; as the uncertainty of the activity with a variance increases, the magnitude of the variance that is acceptable increases. His situation indicates that for a given budgeted activity, the allocated funds were not spent as planned. It is, however, possible that less spending was done because less work was performed. This calls for an investigation beyond noticing the positive variance and rejoicing because R&D has spent less than anticipated. The isolation of construction activity responsible for the positive variance is a necessity if one is to apply corrective actions and use budgets as a controlling mechanism.\\n\\nTo maximize the utilization of resources, the allocation of resources to the various activities must be done on the basis of the blueprint developed by planning. Planning will allow for the optimization of the use of critical skills, especially when it is coupled with some form of project management and/or matrix management. A control system for the release of resources for use can be developed to ensure that resources are available when needed, of a quality and quantity requested in the plan. The release of resources can be done through a formal system of authorization that requires signing and countersigning by authorized personnel. Since the allocation of resources can be done on the basis of a plan, the utilization of resources can be monitored and compared to that plan. The variance of sufficient magnitude can then be examined, and corrective action can be taken (Adler, 2008). More often than not, the organizational structure of a function is set on the basis of historical development, choice of the original but now departed manager and industry custom. This leaves very little freedom for the current manager to make drastic changes in the structure.\\n\\nReferences\\n\\nAdler, N. J. (2008). International Dimensions of Organizational Behavior , 5th edition. Thomson Press.\\n\\nBurkun, S . ( 2005). The Art of Project Management . O’Reilly Media; 1 ed.\\n\\nFrame, J.D. (2002). The New Project Management: Tools for an Age of Rapid Change, Complexity, and Other Business Realities . Jossey-Bass.\\n\\nOwens, I. Wilson, T. (1996). Information and Business Performance: A Study of Information Systems and Services in High Performing Companies . Bowker-Saur.\\n',\n",
       " 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 7e-5\n",
    "num_epochs = 2 # change later\n",
    "batch_size = 64 # change later\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "valid_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# it is recommended to use AdamW for distilBERT models\n",
    "\n",
    "# should we include scheduler?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 38/3212 [00:32<45:24,  1.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_39404\\831553052.py\", line 16, in <module>\n",
      "    for step, (tokens, labels) in enumerate(epoch_iterator):\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"C:\\Users\\berna\\AppData\\Local\\Temp\\ipykernel_39404\\1151847221.py\", line 34, in collate_batch\n",
      "    tokens = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2872, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2958, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3149, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 803, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 770, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 581, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\transformers\\tokenization_utils.py\", line -1, in split\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"d:\\Anaconda3\\envs\\school\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model_du1.parameters(), lr=learning_rate)\n",
    "\n",
    "model_du1.train()\n",
    "\n",
    "train_loss_values = []\n",
    "valid_loss_values = []\n",
    "steps = []\n",
    "val_total_count = 50\n",
    "graph_every = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase \n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Iteration\")\n",
    "    # calculate loss on an epoch level \n",
    "\n",
    "    for step, (tokens, labels) in enumerate(epoch_iterator):\n",
    "        input_ids = tokens['input_ids'].to(model_du1.device)\n",
    "        attention_mask = tokens['attention_mask'].to(model_du1.device)\n",
    "        labels = labels.to(model_du1.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_du1(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % graph_every == 0: # exclude step 0\n",
    "            print('hi')\n",
    "            train_loss_values.append(loss.item())\n",
    "            steps.append(step + epoch * len(epoch_iterator))\n",
    "\n",
    "            # Validation\n",
    "            model_du1.eval()\n",
    "            total_val_loss = 0\n",
    "            val_count = 0\n",
    "            with torch.no_grad():\n",
    "                for val_tokens, val_labels in valid_loader:\n",
    "                    val_input_ids = val_tokens['input_ids'].to(model_du1.device)\n",
    "                    val_attention_mask = val_tokens['attention_mask'].to(model_du1.device)\n",
    "                    val_labels = val_labels.to(model_du1.device)\n",
    "                    val_outputs = model_du1(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
    "                    val_loss = val_outputs.loss\n",
    "                    total_val_loss += val_loss.item()\n",
    "                    val_count += 1\n",
    "                    if val_count > val_total_count:\n",
    "                        break\n",
    "\n",
    "            avg_val_loss = total_val_loss / val_total_count\n",
    "            valid_loss_values.append(avg_val_loss)\n",
    "\n",
    "\n",
    "            # Switch back to training mode\n",
    "            model_du1.train()\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(steps, train_loss_values, label='Training Loss')\n",
    "            plt.plot(steps, valid_loss_values, label='Validation Loss', linestyle='--')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Training & Validation Loss Every {graph_every} Steps')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f'./finetuned_uncased_250kdata_{timestamp}_{epoch}'\n",
    "    model_du1.save_pretrained(model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# Save the model with the learning rate in the filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f'./finetuned_uncased_250kdata_{timestamp}_final'\n",
    "model_du1.save_pretrained(model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/402 [02:05<1:22:06, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    size_test = 10\n",
    "    test_count = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[0]['input_ids'].to(model.device)\n",
    "            attention_mask = batch[0]['attention_mask'].to(model.device)\n",
    "            labels = batch[1].to(model.device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted_labels = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            test_count += 1\n",
    "            if test_count > size_test:\n",
    "                break\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate test accuracy\n",
    "chosen_model = \"finetuned_uncased_250kdata_20240418_155702_final\"\n",
    "model_test = DistilBertForSequenceClassification.from_pretrained(chosen_model)\n",
    "test_accuracy = calculate_accuracy(model_test, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
